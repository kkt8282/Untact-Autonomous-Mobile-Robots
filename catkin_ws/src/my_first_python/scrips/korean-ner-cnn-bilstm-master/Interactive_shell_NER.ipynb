{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:50.621420Z",
     "start_time": "2017-11-20T05:20:49.619093Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from data_utils import Vocabulary\n",
    "from data_utils import load_data_interactive\n",
    "\n",
    "from data_loader import prepare_sequence, prepare_char_sequence, prepare_lex_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from CNN_BiLSTM import CNNBiLSTM\n",
    "from data_loader import get_loader\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:50.633905Z",
     "start_time": "2017-11-20T05:20:50.623178Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_path='./data_in/vocab_ko_NER.pkl'\n",
    "char_vocab_path='./data_in/char_vocab_ko_NER.pkl'\n",
    "pos_vocab_path='./data_in/pos_vocab_ko_NER.pkl'\n",
    "lex_dict_path='./data_in/lex_dict.pkl'\n",
    "model_load_path='./data_in/cnn_bilstm_tagger-179-400_f1_0.8739_maxf1_0.8739_100_200_2.pkl'\n",
    "num_layers=2\n",
    "embed_size=100\n",
    "hidden_size=200 \n",
    "gpu_index=0\n",
    "\n",
    "predict_NER_dict = {0: '<PAD>',\n",
    "                    1: '<START>',\n",
    "                    2: '<STOP>',\n",
    "                    3: 'B_LC',\n",
    "                    4: 'B_DT',\n",
    "                    5: 'B_OG',\n",
    "                    6: 'B_TI',\n",
    "                    7: 'B_PS',\n",
    "                    8: 'I',\n",
    "                    9: 'O'}\n",
    "\n",
    "NER_idx_dic = {'<unk>': 0, 'LC': 1, 'DT': 2, 'OG': 3, 'TI': 4, 'PS': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:50.648788Z",
     "start_time": "2017-11-20T05:20:50.638012Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda(gpu_index)\n",
    "    return Variable(x, volatile=volatile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:51.914303Z",
     "start_time": "2017-11-20T05:20:51.027562Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piai/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# apply word2vec\n",
    "from gensim.models import word2vec\n",
    "pretrained_word2vec_file = './data_in/word2vec/ko_word2vec_' + str(embed_size) + '.model'\n",
    "wv_model_ko = word2vec.Word2Vec.load(pretrained_word2vec_file)\n",
    "word2vec_matrix = wv_model_ko.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:51.971027Z",
     "start_time": "2017-11-20T05:20:51.916650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocab):  28386\n",
      "word2vec_matrix:  (28386, 100)\n"
     ]
    }
   ],
   "source": [
    "# build vocab\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "print(\"len(vocab): \",len(vocab))\n",
    "print(\"word2vec_matrix: \",np.shape(word2vec_matrix))\n",
    "with open(char_vocab_path, 'rb') as f:\n",
    "    char_vocab = pickle.load(f)\n",
    "with open(pos_vocab_path, 'rb') as f:\n",
    "    pos_vocab = pickle.load(f)\n",
    "with open(lex_dict_path, 'rb') as f:\n",
    "    lex_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:52.700063Z",
     "start_time": "2017-11-20T05:20:52.018458Z"
    }
   },
   "outputs": [],
   "source": [
    "# build models\n",
    "cnn_bilstm_tagger = CNNBiLSTM(vocab_size=len(vocab),\n",
    "                                     char_vocab_size=len(char_vocab),\n",
    "                                        pos_vocab_size=len(pos_vocab),\n",
    "                                        lex_ner_size=len(NER_idx_dic),\n",
    "                                        embed_size=embed_size,\n",
    "                                        hidden_size=hidden_size,\n",
    "                                        num_layers=num_layers,\n",
    "                                        word2vec=word2vec_matrix,\n",
    "                                        num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:52.834806Z",
     "start_time": "2017-11-20T05:20:52.708097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't use GPU, you can get error here (in the case of loading state dict from Tensor on GPU)\n",
    "#  To avoid error, you should use options -> map_location=lambda storage, loc: storage. it will load tensor to CPU\n",
    "cnn_bilstm_tagger.load_state_dict(torch.load(model_load_path, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:53.685881Z",
     "start_time": "2017-11-20T05:20:53.563893Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cnn_bilstm_tagger.cuda(gpu_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:54.360263Z",
     "start_time": "2017-11-20T05:20:54.342784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNBiLSTM(\n",
       "  (embed): Embedding(28386, 100, padding_idx=0)\n",
       "  (trainable_embed): Embedding(28386, 100, padding_idx=0)\n",
       "  (lstm): LSTM(818, 200, num_layers=2, batch_first=True, dropout=0.6, bidirectional=True)\n",
       "  (char_embed): Embedding(2284, 100, padding_idx=0)\n",
       "  (pos_embed): Embedding(232, 100, padding_idx=0)\n",
       "  (convs1): ModuleList(\n",
       "    (0): Conv2d(1, 128, kernel_size=(2, 100), stride=(1, 1))\n",
       "    (1): Conv2d(1, 128, kernel_size=(3, 100), stride=(1, 1))\n",
       "    (2): Conv2d(1, 128, kernel_size=(4, 100), stride=(1, 1))\n",
       "    (3): Conv2d(1, 128, kernel_size=(5, 100), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=400, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference mode\n",
    "cnn_bilstm_tagger.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:56.747225Z",
     "start_time": "2017-11-20T05:20:55.514519Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(x_text_batch, x_pos_batch, x_split_batch):\n",
    "    x_text_char_item = []\n",
    "    for x_word in x_text_batch[0]:\n",
    "        x_char_item = []\n",
    "        for x_char in x_word:\n",
    "            x_char_item.append(x_char)\n",
    "        x_text_char_item.append(x_char_item)\n",
    "    x_text_char_batch = [x_text_char_item]\n",
    "\n",
    "    x_idx_item = prepare_sequence(x_text_batch[0], vocab.word2idx)\n",
    "    x_idx_char_item = prepare_char_sequence(x_text_char_batch[0], char_vocab.word2idx)\n",
    "    x_pos_item = prepare_sequence(x_pos_batch[0], pos_vocab.word2idx)\n",
    "    x_lex_item = prepare_lex_sequence(x_text_batch[0], lex_dict)\n",
    "\n",
    "    x_idx_batch = [x_idx_item]\n",
    "    x_idx_char_batch = [x_idx_char_item]\n",
    "    x_pos_batch = [x_pos_item]\n",
    "    x_lex_batch = [x_lex_item]\n",
    "\n",
    "\n",
    "    max_word_len = int(np.amax([len(word_tokens) for word_tokens in x_idx_batch])) # ToDo: usually, np.mean can be applied\n",
    "    batch_size = len(x_idx_batch)\n",
    "    batch_words_len = [len(word_tokens) for word_tokens in x_idx_batch]\n",
    "    batch_words_len = np.array(batch_words_len)\n",
    "\n",
    "    # Padding procedure (word)\n",
    "    padded_word_tokens_matrix = np.zeros((batch_size, max_word_len), dtype=np.int64)\n",
    "    for i in range(padded_word_tokens_matrix.shape[0]):\n",
    "        for j in range(padded_word_tokens_matrix.shape[1]):\n",
    "            try:\n",
    "                padded_word_tokens_matrix[i, j] = x_idx_batch[i][j]\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    max_char_len = int(np.amax([len(char_tokens) for word_tokens in x_idx_char_batch for char_tokens in word_tokens]))\n",
    "    if max_char_len < 5: # size of maximum filter of CNN\n",
    "        max_char_len = 5\n",
    "        \n",
    "    # Padding procedure (char)\n",
    "    padded_char_tokens_matrix = np.zeros((batch_size, max_word_len, max_char_len), dtype=np.int64)\n",
    "    for i in range(padded_char_tokens_matrix.shape[0]):\n",
    "        for j in range(padded_char_tokens_matrix.shape[1]):\n",
    "            for k in range(padded_char_tokens_matrix.shape[1]):\n",
    "                try:\n",
    "                    padded_char_tokens_matrix[i, j, k] = x_idx_char_batch[i][j][k]\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "    # Padding procedure (pos)\n",
    "    padded_pos_tokens_matrix = np.zeros((batch_size, max_word_len), dtype=np.int64)\n",
    "    for i in range(padded_pos_tokens_matrix.shape[0]):\n",
    "        for j in range(padded_pos_tokens_matrix.shape[1]):\n",
    "            try:\n",
    "                padded_pos_tokens_matrix[i, j] = x_pos_batch[i][j]\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    # Padding procedure (lex)\n",
    "    padded_lex_tokens_matrix = np.zeros((batch_size, max_word_len, len(NER_idx_dic)))\n",
    "    for i in range(padded_lex_tokens_matrix.shape[0]):\n",
    "        for j in range(padded_lex_tokens_matrix.shape[1]):\n",
    "            for k in range(padded_lex_tokens_matrix.shape[2]):\n",
    "                try:\n",
    "                    for x_lex in x_lex_batch[i][j]:\n",
    "                        k = NER_idx_dic[x_lex]\n",
    "                        padded_lex_tokens_matrix[i, j, k] = 1\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "                \n",
    "    x_text_batch = x_text_batch\n",
    "    x_split_batch = x_split_batch\n",
    "    padded_word_tokens_matrix = torch.from_numpy(padded_word_tokens_matrix)\n",
    "    padded_char_tokens_matrix = torch.from_numpy(padded_char_tokens_matrix)\n",
    "    padded_pos_tokens_matrix = torch.from_numpy(padded_pos_tokens_matrix)\n",
    "    padded_lex_tokens_matrix = torch.from_numpy(padded_lex_tokens_matrix).float()\n",
    "    lengths = batch_words_len\n",
    "\n",
    "    return x_text_batch, x_split_batch, padded_word_tokens_matrix, padded_char_tokens_matrix, padded_pos_tokens_matrix, padded_lex_tokens_matrix, lengths\n",
    "\n",
    "def parsing_seq2NER(argmax_predictions, x_text_batch):\n",
    "    predict_NER_list = []\n",
    "    predict_text_NER_result_batch = copy.deepcopy(x_text_batch[0]) #tuple ([],) -> return first list (batch_size == 1)\n",
    "    for argmax_prediction_seq in argmax_predictions:\n",
    "        predict_NER = []\n",
    "        NER_B_flag = None # stop B\n",
    "        prev_NER_token = None\n",
    "        for i, argmax_prediction in enumerate(argmax_prediction_seq):\n",
    "                now_NER_token = predict_NER_dict[argmax_prediction.cpu().data.numpy()[0]]\n",
    "                predict_NER.append(now_NER_token)\n",
    "                if now_NER_token in ['B_LC', 'B_DT', 'B_OG', 'B_TI', 'B_PS'] and NER_B_flag is None: # O B_LC\n",
    "                    NER_B_flag = now_NER_token # start B\n",
    "                    predict_text_NER_result_batch[i] = '<'+predict_text_NER_result_batch[i]\n",
    "                    prev_NER_token = now_NER_token\n",
    "                    if i == len(argmax_prediction_seq)-1:\n",
    "                        predict_text_NER_result_batch[i] = predict_text_NER_result_batch[i]+':'+now_NER_token[-2:]+'>'\n",
    "\n",
    "                elif now_NER_token in ['B_LC', 'B_DT', 'B_OG', 'B_TI', 'B_PS'] and NER_B_flag is not None: # O B_LC B_DT\n",
    "                    predict_text_NER_result_batch[i-1] = predict_text_NER_result_batch[i-1]+':'+prev_NER_token[-2:]+'>'\n",
    "                    predict_text_NER_result_batch[i] = '<' + predict_text_NER_result_batch[i]\n",
    "                    prev_NER_token = now_NER_token\n",
    "                    if i == len(argmax_prediction_seq)-1:\n",
    "                        predict_text_NER_result_batch[i] = predict_text_NER_result_batch[i]+':'+now_NER_token[-2:]+'>'\n",
    "\n",
    "                elif now_NER_token in ['I'] and NER_B_flag is not None:\n",
    "                    if i == len(argmax_prediction_seq) - 1:\n",
    "                        predict_text_NER_result_batch[i] = predict_text_NER_result_batch[i] + ':' + NER_B_flag[-2:] + '>'\n",
    "\n",
    "                elif now_NER_token in ['O'] and NER_B_flag is not None: # O B_LC I O\n",
    "                    predict_text_NER_result_batch[i-1] = predict_text_NER_result_batch[i-1] + ':' + prev_NER_token[-2:] + '>'\n",
    "                    NER_B_flag = None # stop B\n",
    "                    prev_NER_token = now_NER_token\n",
    "\n",
    "        predict_NER_list.append(predict_NER)\n",
    "    return predict_NER_list, predict_text_NER_result_batch\n",
    "\n",
    "def generate_text_result(text_NER_result_batch, x_split_batch):\n",
    "    prev_x_split = 0 \n",
    "    text_string = ''\n",
    "    for i, x_split in enumerate(x_split_batch[0]):\n",
    "        if prev_x_split != x_split:\n",
    "            text_string = text_string+' '+text_NER_result_batch[i]\n",
    "            prev_x_split = x_split\n",
    "        else:\n",
    "            text_string = text_string +''+ text_NER_result_batch[i]\n",
    "            prev_x_split = x_split\n",
    "    return text_string\n",
    "\n",
    "\n",
    "def NER_print(input_str):\n",
    "    input_str.replace(\"  \", \"\")\n",
    "    input_str = input_str.strip()\n",
    "    \n",
    "    x_text_batch, x_pos_batch, x_split_batch = load_data_interactive(input_str)\n",
    "    x_text_batch, x_split_batch, padded_word_tokens_matrix, padded_char_tokens_matrix, padded_pos_tokens_matrix, padded_lex_tokens_matrix, lengths = preprocessing(x_text_batch, x_pos_batch, x_split_batch)\n",
    "    \n",
    "    # Test\n",
    "    argmax_labels_list = []\n",
    "    argmax_predictions_list = []\n",
    "\n",
    "\n",
    "    padded_word_tokens_matrix = to_var(padded_word_tokens_matrix, volatile=True)\n",
    "    padded_char_tokens_matrix = to_var(padded_char_tokens_matrix, volatile=True)\n",
    "    padded_pos_tokens_matrix = to_var(padded_pos_tokens_matrix, volatile=True)\n",
    "    padded_lex_tokens_matrix = to_var(padded_lex_tokens_matrix, volatile=True)\n",
    "\n",
    "\n",
    "    predictions = cnn_bilstm_tagger.sample(padded_word_tokens_matrix, padded_char_tokens_matrix, padded_pos_tokens_matrix, padded_lex_tokens_matrix, lengths)\n",
    "    \n",
    "    max_predictions, argmax_predictions = predictions.max(2)\n",
    "\n",
    "    if len(argmax_predictions.size()) != len(\n",
    "        predictions.size()):  # Check that class dimension is reduced or not (API version issue, pytorch 0.1.12)\n",
    "        max_predictions, argmax_predictions = predictions.max(2, keepdim=True)\n",
    "\n",
    "    argmax_predictions_list.append(argmax_predictions)\n",
    "    \n",
    "    predict_NER_list, predict_text_NER_result_batch = parsing_seq2NER(argmax_predictions, x_text_batch)\n",
    "    \n",
    "    origin_text_string = generate_text_result(x_text_batch[0], x_split_batch)\n",
    "    predict_NER_text_string = generate_text_result(predict_text_NER_result_batch, x_split_batch)\n",
    "\n",
    "    print(\"output> \",predict_NER_text_string)\n",
    "    print(\"\")\n",
    "    \n",
    "    return predict_text_NER_result_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:29:12.848154Z",
     "start_time": "2017-11-20T05:26:33.859211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> 코로나 확진자 몇명이야\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piai/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output>  코로나 확진자 몇명이야\n",
      "\n",
      "['코로나', '확진', '자', '몇', '명', '이', '야']\n"
     ]
    }
   ],
   "source": [
    "# 모델성능 f1 87.39\n",
    "while(True):\n",
    "    \n",
    "    input_str = input('input> ')\n",
    "    \n",
    "    if input_str == 'exit':\n",
    "        break\n",
    "    else:\n",
    "        prediction_string_list = NER_print(input_str)\n",
    "        print(prediction_string_list)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
